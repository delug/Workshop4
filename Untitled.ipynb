{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: \n",
    "\n",
    "Neural Networks (NN) are a very broad category of algorithms and models whose designs were inspired by the patterns and complexities of the human mind. The design philosophy behind NN is fairly simple: iteratively stack relatively simple components to build a significantly more complex model.\n",
    "\n",
    "Mathematically, NN are considered functions from one space to another. Hence, their operation greatly depends on the concepts of Jacobians, Chain Rule, Computational Graphs, all of which are collectively applied in a technique known as backpropagation. \n",
    "\n",
    "## Perceptrons (Binary Classifier)\n",
    "\n",
    "To begin with, we'll define and discuss the first and simplest component of Neural Networks: a perceptron. Perceptrons are basic binary classifierss which serve as a conceptual basis for Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Perceptron](images/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we see above. First, we have our inputs $x_1,x_2,\\dots,x_n$. These are whatever variables or features you will be working with, alongside an extra constant $w_0$ known as a *bias*. Generally the bias is optional, but favored. Each input, including the bias, is  multiplied against its own personal *weight* $w_1,w_2,\\dots,w_n$. These weights are also known as parameters, and we'll be using them interchangeably. The products of inputs and weights is then summed together to get $\\sum_{i=1}^nx_iw_i$, and finally that total is *activated*, which means it has an *activation function* applied to it. In the case of a vanilla perceptron, the activation function is a basic step function: \n",
    "\n",
    "$$\n",
    "\\sigma:\\mathbb{R}\\to\\mathbb{R}\\\\ \n",
    "\\sigma(x)=0\\text{ if }x\\leq 0\\text{ otherwise }\\sigma(x)=1\n",
    "$$\n",
    "\n",
    "Generally though this step function doesn't see actual use. Instead, the sigmoid function is favored: \n",
    "$$\n",
    "\\sigma:\\mathbb{R}\\to\\mathbb{R}\\\\ \n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$ \n",
    "\n",
    "It serves a very similar purpose, but unlike a step function, its smooth everywhere. This means that it's differentiable, which will be key for the model.\n",
    "\n",
    "\n",
    "So let's formalize what our perceptron does mathematically. Let's start without a bias. We can consider our inputs as a vector $\\vec x\\in\\mathbb{R}^n$ where $\\vec x=[x_1,x_2,\\dots,x_n]^T$, and similarly our weights as a vector: $\\vec w\\in\\mathbb{R}^n$ with $\\vec w=[w_1,w_2,\\dots,w_n]^T$ then we get that our weighted sum is $\\sum_{i=1}^nx_iw_i=\\vec x\\cdot\\vec w= \\vec w^T\\vec x$. If we include our bias, the only difference we make to this equation is that we define $\\vec x=[1,x_1,x_2,\\dots,x_n]^T$ and our weights as $\\vec w=[w_0,w_1,w_2,\\dots,w_n]^T$. This definition becomes especially convenient later when building up large Neural Networks. \n",
    "\n",
    "We can consider our perceptron as a function \n",
    "$$\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R}\\\\\n",
    "F(\\vec x,\\vec w)=\\sigma(\\vec w^T\\vec x)\n",
    "$$\n",
    "\n",
    "The real question is now: how do we figure out our weights? Well we start by defining some loss function. For simplicity, we'll be using an $L^2$ loss function. For every input $\\vec x_i$ there is an accompanying value known as a *label* or *target* which is what we hope our model will output, written as $y_i\\in\\mathbb{R}$. Then the loss for a particular input is \n",
    "$$\n",
    "L:\\mathbb{R}^n\\to\\mathbb{R}\\\\\n",
    "L(\\vec x_i, \\vec w)=(F(\\vec x_i,\\vec w)-y_i)^2\n",
    "$$\n",
    "\n",
    "This is essentially the *distance* between the model output $\\hat x_i=F(\\vec x_i,\\vec w)$ and the target $y_i$. Now we want to minimize the distance between those two values, so we can apply gradient descent! In order to do that, first we must figure out what the gradient is. As a reminder, what we want is the Jacobian of $L$ with respect to $\\vec w$, written as $(\\mathcal{D}L)$. Note we'll be omitting the arguments to the functions, since those won't change until the last couple steps. Let's worth that out.\n",
    "\n",
    "$$\n",
    "\\mathcal{D}L=\\mathcal{D}[(F-y_i)^2]\n",
    "\\\\=2(F-y_i)*\\mathcal{D}[F-y_i]\n",
    "\\\\=2(F-y_i)*(\\mathcal{D}[F]-\\mathcal{D}[y_i])\n",
    "\\\\=2(F-y_i)*\\mathcal{D}[F]\n",
    "$$\n",
    "\n",
    "So now in particular, we'll focus on the $\\mathcal{D}[F]$ term. \n",
    "\n",
    "**Note**: For convenience, we will set a variable $S=\\vec w^T\\vec x=\\vec x^T\\vec w$ \n",
    "$$\n",
    "\\mathcal{D}[F]=\\mathcal{D}[\\sigma(S)]\n",
    "=(\\mathcal{D}[\\sigma])(S)\\mathcal{D}[S]\n",
    "=(\\sigma(S)(1-\\sigma(S))\\vec x^T\n",
    "$$\n",
    "\n",
    "Plugging this back into our equation for \n",
    "$$\n",
    "\\mathcal{D}[L]=2(F-y_i)*\\mathcal{D}[F]=2(F-y_i)*(\\sigma(S)(1-\\sigma(S))\\vec x^T\n",
    "\\\\=2(\\sigma(S)-y_i)*(\\sigma(S)(1-\\sigma(S))\\vec x^T\n",
    "$$\n",
    "\n",
    "Let's take a step back. Remember, we want to figure out *how much to change* $\\vec w$ in order to minimize $L$. Now that we know $\\mathcal{D}[L]$, we know the direction of *greatest increase*, so to minimize it, we only need to step in the opposite direction, $-\\mathcal{D}[L]$. So we know the direction, but how large should our step be? The smaller the step, the more carefuly the algorithm will hone in on its closest local minima. This leads to slower, less noisy training. Setting the step size to be too low will lead to the model spending way too much time training, and likely getting stuck in the first local minima it finds. In practice, this can lead to very poor models. A larger step size leads to faster, noisier training. This noise can *knock* the model out of a local minima, causing it to optimize in other directions. Setting the learning rate to be too large can lead to unstable, non-reproducible training that either diverges, or fails to converge to any minima at all.\n",
    "\n",
    "Unfortunately, there's no golden rule to setting learning rate. Repeated experiments are really the only way to tell what work. There are some heuristics however. The larger the batch size, the greater you want your learning rate to be. If you don't intend to train the model for a long time, then longer steps are encouraged. It generally depends on the exact problem and ciscumstances of your model.\n",
    "\n",
    "For now, let's say that our learning rate $\\epsilon=10^{-3}$. Then in order to update our weights, we will set $w_{new}=w_{old}-\\epsilon\\mathcal{D}[L]$. This seems very small, but generally this process will be repeated anywhere from millions to billions of times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally speaking, we run through the following process called training:\n",
    "+ Calculate $\\hat y=F(\\vec x,\\vec w)=\\sigma(\\vec w^T\\vec x)$\n",
    "+ Calculate $\\mathcal{D}[L]$\n",
    "+ Update weights using $w_{new}=w_{old}-\\epsilon\\mathcal{D}[L]$\n",
    "+ Repeat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classifier\n",
    "\n",
    "Consider an example problem of handwritten digit classification. That is to say, you're provided with a $28x28$ image which depicts some digit $0-9$ on it, and your job is to build a model to determine which digit it is. For example:\n",
    "\n",
    "![MNIST](images/MNISTExample.png)\n",
    "\n",
    "\n",
    "This is a very common *first task* in machine learning and deep learning. It's fairly easy to get an *adequate* to *good* model for this task, but difficult to get an *amazing* model, so it is a favored task by those starting out in deep learning. Today we'll tackle this task using our perceptrons! Now, originally a perceptron is only a binary classifier, but the MNIST dataset has 10 classes -- one for each digit. The solution to this is to set up 10 binary classifiers in parallel, with each one making a binary classification as to whether or not the image is a certain digit. For example, perceptron 4 is a binary classifier on whether or not the image is a 4. Perceptron 6 decides whether or not the image is a 6. Each perceptron will take in the $28x28$ image in the form of a $784$-dimensional vector (note: $28x28=784$)\n",
    "\n",
    "Usually a perceptron's classification is **YES** if the result is $\\geq .5$ and **NO** otherwise. The issue with a multi-perceptron setup like the one we'll use is that we can have *more than one* perceptron with a positive classification. To solve this, we must remember what the resulting value actually represents: the models *confidence* regarding its choice. Using this, for the multiclass classifier, we simply take the *largest* value of the various perceptrons.\n",
    "\n",
    "With that being said, it can still lead to issues where we have multiple high-confidence perceptrons, meaning that the sum of all end confidences can be greater than 1. That means that we can no longer consider them probabilities. To fix this, and normalize the outputs, we apply the **softmax** function $$softmax(z)_i=\\frac{z_i}{\\sum_{j=1}^kz_j}$$ We won't discuss the semantincs of the function too much, but just think of it as a function which transforms regular perceptron outputs into a multi-class probability.\n",
    "\n",
    "Let's formalize what weve discussed. Before, we had a function to represent our network as:\n",
    "$$\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R}\\\\\n",
    "F(\\vec x,\\vec w)=\\sigma(\\vec w^T\\vec x)\n",
    "$$\n",
    "\n",
    "but now we need our function to instead look like\n",
    "$$\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R}^{10}\\\\\n",
    "$$\n",
    "\n",
    "where the output dimensions equal the number of classes we have. Then, for $y=F(\\vec x, W)$, we get that $y_i=$the probability that the image $x$ represents the digit $i$. Notice I represent the weights with a matrix $W$ instead of a vector this time. When we consider the function $F_i$, which gives us $y_i$ using a weight vector $\\vec w_i$. Our final output vector $\\vec y=[\\sigma(\\vec w_1^T\\vec x),\\dots,\\sigma(\\vec w_{10}^T\\vec x)]^T=\\sigma(Wx)$ where $W\\in\\mathbb{R}^{10\\times n}$ and $x\\in\\mathbb{R}^n=\\mathbb{R}^{n\\times 1}$\n",
    "\n",
    "The general formulation of a multi-class network with $n$ input features and $m$ output classes is\n",
    "$$\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R}^{m}\\\\\n",
    "F(\\vec x,W)=\\sigma(W\\vec x)\n",
    "$$\n",
    "\n",
    "Now let's try to find $\\mathcal{D}[L]$ with respect to $W$. Now, it turns out to be significantly simpler to instead focus on finding the Jacobian with respect to every individual $\\vec w_i$, but keep in mind it ends up being equivelant. \n",
    "\n",
    "$$\n",
    "L=\\|F-y\\|^2\n",
    "\\\\\\mathcal{D}[L]=2(F_i-y_i)\\mathcal{D}[F_i-y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i-y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i]-\\mathcal{D}[y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i]\n",
    "$$\n",
    "\n",
    "Taking a look at $\\mathcal{D}[F_i]$ in particular, setting $S=w_i^T\\vec x$ for convenience,\n",
    "\n",
    "$$\n",
    "\\mathcal{D}[F_i]=\\mathcal{D}[\\sigma(S)]\n",
    "\\\\=\\mathcal{D}[\\sigma](S)*\\mathcal{D}[S]\n",
    "\\\\=\\sigma(S)(1-\\sigma(S))*\\mathcal{D}[S]\n",
    "\\\\=\\sigma(S)(1-\\sigma(S))\\vec x^T\n",
    "$$\n",
    "\n",
    "therefore we have that \n",
    "$$\n",
    "\\\\\\mathcal{D}[L]=2(F_i-y_i)\\mathcal{D}[F_i-y_i]\n",
    "\\\\=2(\\sigma(w_i^T\\vec x)-y_i)\\sigma(w_i^T\\vec x)(1-\\sigma(w_i^T\\vec x))\\vec x^T\n",
    "\\\\=2(F_i-y_i)F_i(1-F_i)\\vec x^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Network\n",
    "\n",
    "All together, our model is still very simple. Simple models tend to fail in modeling the complexities of cecrtain data. It turns out, that includes the MNIST classification task! So, to increase our complexity, we'll add a new layer, known as a *hidden layer*. We refer to it as hidden because the values at those perceptrons don't necessarily hold physical significane. They need only be computationally relevent, but aren't required to make semantic sense. Hence, we generally don't directly observe them, and call them *hidden*. With this, we'll start referring to each perceptron as a *node*.\n",
    "\n",
    "Let's formulate a Neural Network with 1 *hidden layer* with $l$ *nodes*. How does this hidden layer actually come into play, mathematically and computationally? It can be thought of as an intermediate step. Where before we had \n",
    "\n",
    "$$\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R^m}\\\\\n",
    "F(\\vec x,W)=\\sigma(W\\vec x)\n",
    "$$\n",
    "\n",
    "We now instead have: \n",
    "\n",
    "$$\n",
    "G: \\mathbb{R}^n\\to\\mathbb{R}^l\\\\\n",
    "G(\\vec x,W_1)=\\sigma(W_1\\vec x) \\text{ where } W_1\\in\\mathbb{R}^{l\\times n}\\\\\n",
    "\\\\\n",
    "H: \\mathbb{R}^l\\to\\mathbb{R^m}\\\\\n",
    "H(\\vec x,W_2)=\\sigma(W_2\\vec x) \\text{ where } W_2\\in\\mathbb{R}^{m\\times l}\\\\\n",
    "\\\\\n",
    "\\text{hence}\\\\\n",
    "\\\\\n",
    "F: \\mathbb{R}^n\\to\\mathbb{R^m}\\\\\n",
    "F(\\vec x, W_1,W_2)=H\\circ G\n",
    "\\\\=H(G(\\vec x, W_1), W_2)\n",
    "\\\\=H(\\sigma(W_1\\vec x), W_2)\n",
    "\\\\=\\sigma(W_2\\sigma(W_1\\vec x))\n",
    "$$\n",
    "\n",
    "This is how we build **deep** networks. We repeatedly apply these relatively simple layers, building up a progressively more complex representation until we reach a sufficient amount of complexity for our particular task.\n",
    "\n",
    "To review, a simple perceptron can be considered a *node*. Multiple nodes when placed together are referred to as *layers*. The layers excluding the input and output are referred to as *hidden layers*. When stacking multiple hidden layers on top of each other, we build a *Deep Neural Network*.\n",
    "\n",
    "This is still only the tip of the iceberg, and we'll explore all the nuances and crazy architectures which stem from this basic principle of repeatedly stacking on complexities to build a model, but for now, we'll dive a bit deeper into the mathematics of it all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Backpropagation is the heart of deep learning. Put simply, backpropogation is following the error in a system, and assigning blame where blame is due. Even more simply, it's the chain rule.\n",
    "\n",
    "Think back to how we optimized our single perceptron. To optimize a parameter $p$, you need the gradient of $L$ with respect to $p$. In a more general sense, you need the Jacobian since we'll be dealing with tensor parameters, not just vectors. So in our above model with a single hidden layer, we now need two different values: $\\mathcal{D}[L]$ with respect to $W_1$ AND $W_2$ Let's try to calculate that. Similarly to the no hidden layer case, we will prefer to look at a single row of our weights at a time. We'll start with respect to $(W_2)_i=\\vec w_{2,i}$\n",
    "\n",
    "$$\n",
    "L=\\|F-y\\|^2\n",
    "\\\\\\mathcal{D}[L]=2(F_i-y_i)\\mathcal{D}[F_i-y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i-y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i]-\\mathcal{D}[y_i]\n",
    "\\\\=2(F_i-y_i)\\mathcal{D}[F_i]\n",
    "$$\n",
    "\n",
    "Taking a look at $\\mathcal{D}[F_i]$ in particular, setting $S_i=\\vec w_{2,i}F_1$ for convenience,\n",
    "\n",
    "$$\n",
    "\\mathcal{D}[F_i]\n",
    "\\\\=\\mathcal{D}[\\sigma(S_i)]\n",
    "\\\\=\\mathcal{D}[\\sigma](S_i)*\\mathcal{D}[S_i]\n",
    "\\\\=\\sigma(S_i)(1-\\sigma(S_i))F_1\n",
    "$$\n",
    "\n",
    "\n",
    "Notice that the Jacobians for the weights in $W_2$ stops at $F_1$. That's because, with respect to $W_2\\text{, }F_1$ is constant! But that also leads into an interesting system. Now $F$ depends on $W_2$, which depends on $F_1$, and that subsequently depends on $W_1$. That means that when we want to figure out the weights $W_1$, it suffices to consider that \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\vec w_{1,i}}=\\frac{\\partial L}{\\partial W_2}\\frac{\\partial W_2}{\\partial w_{1,i}}\n",
    "$$\n",
    "This is just an application of the chain rule, and the premise of Backpropagation. What this really means is that, since we have to compute $\\frac{\\partial L}{W_2}$ anyways, we can save that information to speed up the computation of $\\frac{\\partial L}{\\partial \\vec W_1}$ This may seem trivial in this small scale, but when running computations for **millions** of weights, this brings the problem from the realm of impossibility to the realm of smart-phone-compatible-computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
